{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subutayebru/3DCV/blob/main/Advanced_Calibration_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6FjuE1Aej3X"
      },
      "source": [
        "# Welcome to the Calibration Workshop!\n",
        "In this new workshop, you're going to learn about camera calibration in an advanced way. We often see calibration as an easy process: take a few images of a checkerboard, use an OpenCV() function, calibrate!\n",
        "\n",
        "The reality is often very different, and in this workshop, I want to start calibration with the end in mind. This means when we'll calibrate, **we'll be able to see the 3D Reconstruction, and the impact of calibration on the end result**.\n",
        "\n",
        "Ready? Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G5en5XMehQ7"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d22tAszzZk6j",
        "collapsed": true,
        "outputId": "200196c7-62a7-4639-d838-b13766775521",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open3d==0.19.0\n",
            "  Downloading open3d-0.19.0-cp312-cp312-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from open3d==0.19.0) (2.0.2)\n",
            "Collecting dash>=2.6.0 (from open3d==0.19.0)\n",
            "  Downloading dash-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from open3d==0.19.0) (3.1.3)\n",
            "Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from open3d==0.19.0) (3.1.2)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.12/dist-packages (from open3d==0.19.0) (5.10.4)\n",
            "Collecting configargparse (from open3d==0.19.0)\n",
            "  Downloading configargparse-1.7.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d==0.19.0)\n",
            "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting addict (from open3d==0.19.0)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.12/dist-packages (from open3d==0.19.0) (11.3.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.12/dist-packages (from open3d==0.19.0) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.12/dist-packages (from open3d==0.19.0) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from open3d==0.19.0) (6.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.12/dist-packages (from open3d==0.19.0) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open3d==0.19.0) (4.67.1)\n",
            "Collecting pyquaternion (from open3d==0.19.0)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d==0.19.0) (5.24.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d==0.19.0) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d==0.19.0) (4.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d==0.19.0) (2.32.4)\n",
            "Collecting retrying (from dash>=2.6.0->open3d==0.19.0)\n",
            "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d==0.19.0) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d==0.19.0) (75.2.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d==0.19.0) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d==0.19.0) (8.3.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d==0.19.0) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d==0.19.0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d==0.19.0) (3.0.3)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d==0.19.0)\n",
            "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.4->open3d==0.19.0) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.4->open3d==0.19.0) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.4->open3d==0.19.0)\n",
            "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.4->open3d==0.19.0) (3.0.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d==0.19.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d==0.19.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d==0.19.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d==0.19.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d==0.19.0) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d==0.19.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d==0.19.0) (2.9.0.post0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7.0->open3d==0.19.0) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7.0->open3d==0.19.0) (4.25.1)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7.0->open3d==0.19.0) (5.8.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0->open3d==0.19.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0->open3d==0.19.0) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21->open3d==0.19.0) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21->open3d==0.19.0) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21->open3d==0.19.0) (3.6.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.19.0) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.19.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.19.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.19.0) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.19.0) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.19.0) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.19.0) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.19.0) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d==0.19.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d==0.19.0) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d==0.19.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d==0.19.0) (0.27.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d==0.19.0) (4.4.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d==0.19.0) (8.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d==0.19.0) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->dash>=2.6.0->open3d==0.19.0) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d==0.19.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d==0.19.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d==0.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d==0.19.0) (2025.8.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.19.0) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.19.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.19.0) (0.2.14)\n",
            "Downloading open3d-0.19.0-cp312-cp312-manylinux_2_31_x86_64.whl (447.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-3.2.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading configargparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
            "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: addict, widgetsnbextension, retrying, pyquaternion, configargparse, comm, ipywidgets, dash, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed addict-2.4.0 comm-0.2.3 configargparse-1.7.1 dash-3.2.0 ipywidgets-8.1.7 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.4.2 widgetsnbextension-4.0.14\n",
            "Requirement already satisfied: pythreejs==2.4.2 in /usr/local/lib/python3.12/dist-packages (2.4.2)\n",
            "Requirement already satisfied: ipywidgets>=7.2.1 in /usr/local/lib/python3.12/dist-packages (from pythreejs==2.4.2) (8.1.7)\n",
            "Requirement already satisfied: ipydatawidgets>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from pythreejs==2.4.2) (4.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pythreejs==2.4.2) (2.0.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.12/dist-packages (from pythreejs==2.4.2) (5.7.1)\n",
            "Requirement already satisfied: traittypes>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipydatawidgets>=1.1.1->pythreejs==2.4.2) (0.2.1)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=7.2.1->pythreejs==2.4.2) (0.2.3)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=7.2.1->pythreejs==2.4.2) (7.34.0)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=7.2.1->pythreejs==2.4.2) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=7.2.1->pythreejs==2.4.2) (3.0.15)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=7.2.1->pythreejs==2.4.2) (0.2.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install open3d==0.19.0\n",
        "!pip install pythreejs==2.4.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddE1r3BGewJf"
      },
      "outputs": [],
      "source": [
        "import sys, os, cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "import open3d as o3d\n",
        "import pythreejs as p3js\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "IuewFNFVIkRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://stereo-vision.s3.eu-west-3.amazonaws.com/calib_data.zip && unzip calib_data.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eSVYEknEG1d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Visualize the folders & images\n",
        "As you can see, we have **2 charuco folders** (calibration images); one \"good\" and one \"bad\". Yes, this means we purposely screwed up the images, took them a bit blurry, incomplete etc... so you can see the difference.<p>\n",
        "Then, we have a folders with robot picture we took in Taiwan near a content creator's house: **y\n",
        "/robot**"
      ],
      "metadata": {
        "id": "7K7bwkFtHxH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Generate a Charuco board\n",
        "The purpose of camera calibration is to determine the internal parameters (intrinsics) and the relationship between two cameras (extrinsics) in a stereo setup. This helps in correcting distortion, aligning the cameras, and measuring distances in 3D from stereo images.\n",
        "\n",
        "For a stereo camera like the [ZED 2](https://www.stereolabs.com/en-fr/products/zed-2) (used in this workshop), calibration is crucial to ensure that both the left and right cameras are correctly aligned, which allows for more accurate depth perception and 3D reconstruction.\n",
        "\n",
        "So what is the first step?\n",
        "\n",
        "**Yes, the first step is to generate a calibration board.**\n",
        "<p>\n",
        "Charuco?\n",
        "Yes, if you learned calibration, you probably learned via checkerboards. A charuco board isn't too different. It has a calibration pattern made up of a grid of squares (like a traditional chessboard) combined with small ArUco markers inside some squares. ArUco markers are small and easily detectable patterns that help find precise corner locations, even in low-quality or partially occluded images. This makes Charuco boards more reliable for calibration than a regular chessboard.\n",
        "<p>\n",
        "> Charuco=Chessboard + Aruco\n",
        "\n",
        "\n",
        "\n",
        "Now the question is, why generate one? Aren't there any good ones online? Yes, there are, but in calibration, you want to be precise. If your board is printed on a paper, it's different than if it's shown in a TV. And even there, if it's full screen, the board size will be different than if it's not full screen."
      ],
      "metadata": {
        "id": "Ydvmh70QLM1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First, we define the charuco board.**<p>\n",
        "It's done in a few steps:\n",
        "1. **Step 1**: Define a dictionary of ArUco markers to be used (DICT_4X4_250 means 4x4 markers with up to 250 unique IDs).\n",
        "2. **Step 2**: Calculate the size of the Charuco board in meters, based on the size of your screen (27-inch display) and the desired size of each square (0.04 meters).\n",
        "3. **Step 3**: Generate the Charuco board with the number of squares (14x7 grid), and display it on the screen or save it for printing.\n",
        "\n",
        "> For the data collected for this colab, the board was displayed on a 27-inch screen.\n"
      ],
      "metadata": {
        "id": "HIO_PtsvMY6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chessboard configuration\n",
        "aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_250)\n",
        "\n",
        "# Screen dimensions and resolution\n",
        "screen_diagonal_inches = 27\n",
        "screen_resolution = (1920, 1080)\n",
        "screen_aspect_ratio = 16 / 9\n",
        "\n",
        "# Convert screen diagonal to meters\n",
        "screen_diagonal_meters = screen_diagonal_inches * 0.0254\n",
        "\n",
        "# Calculate screen width and height in meters\n",
        "screen_height_meters = screen_diagonal_meters * (9 / np.sqrt(16**2 + 9**2))\n",
        "screen_width_meters = screen_diagonal_meters * (16 / np.sqrt(16**2 + 9**2))\n",
        "\n",
        "# Define the desired size of each square in meters\n",
        "square_size_meters = 0.04  # example size in meters\n",
        "\n",
        "# Calculate DPI (dots per meter)\n",
        "dpm = screen_resolution[0] / screen_width_meters\n",
        "\n",
        "# Calculate the size of each square in pixels\n",
        "square_size_pixels = int(square_size_meters * dpm)\n",
        "\n",
        "# Define the size of the Charuco board\n",
        "squares_x = 14 # number of squares along the x-axis\n",
        "squares_y = 7   # number of squares along the y-axis"
      ],
      "metadata": {
        "id": "4v2o5qqlLMMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Then, we generate and visualize it**.<p>"
      ],
      "metadata": {
        "id": "9JK0NvI1Mbv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Charuco board\n",
        "board = cv2.aruco.CharucoBoard((squares_x, squares_y), square_size_pixels / dpm, (square_size_pixels / dpm) * 0.75, aruco_dict)\n",
        "\n",
        "# Generate the image\n",
        "image_size = (screen_resolution[0], screen_resolution[1])  # match the screen resolution\n",
        "image = board.generateImage(image_size, marginSize=0, borderBits=1)\n",
        "\n",
        "# Display the Charuco board\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Save the image if needed\n",
        "cv2.imwrite('output/charuco_board_27inch_14x7.png', image)"
      ],
      "metadata": {
        "id": "n30rKngeMfJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the object points from the Charuco board\n",
        "charuco_object_points = board.getChessboardCorners()\n",
        "num_points = len(charuco_object_points)\n",
        "print(num_points)"
      ],
      "metadata": {
        "id": "g-WbT6GKPHmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Calibration\n",
        "We now enter the calibration process, we're going to begin with defining **image points** and **object points**. Image points are in pixels, object points are the real 3D points."
      ],
      "metadata": {
        "id": "wy7ftyucQomG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frameSize = (1920, 1080)\n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "objp = np.array(charuco_object_points)  # Shape: (78, 3)\n",
        "\n",
        "# Arrays to store object points and image points from all the images.\n",
        "objpoints = [] # 3d point in real world space\n",
        "imgpointsL = [] # 2d points in image plane.\n",
        "imgpointsR = [] # 2d points in image plane.\n",
        "objpointsL = [] # 3d point in real world space\n",
        "objpointsR = [] # 3d point in real world space\n",
        "\n",
        "idsL = []\n",
        "cornersL = []\n",
        "imagesL = []\n",
        "idsR = []\n",
        "cornersR = []\n",
        "imagesR = []\n",
        "\n",
        "imagesLeft = []\n",
        "imagesRight = []\n",
        "\n",
        "parameters =  cv2.aruco.DetectorParameters()\n",
        "parameters.cornerRefinementMethod = cv2.aruco.CORNER_REFINE_CONTOUR"
      ],
      "metadata": {
        "id": "ciPigahFQsOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: This creates a Charuco detector object that is used to detect both ArUco markers and chessboard corners.\n",
        "* Parameters:\n",
        "* board: The Charuco board used for calibration.\n",
        "* detectorParams: The detection parameters configured earlier, such as corner refinement.\n",
        "* Usage in Code: This detector helps in finding corners and IDs of the ArUco markers on the Charuco board from the calibration images."
      ],
      "metadata": {
        "id": "DbFLVkW6RS4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\color{#BB5544}{\\text{TODO: Define a Charuco detector using the cv2.aruco.CharucoDetecto() function. Fill in the parameters.}}$\n"
      ],
      "metadata": {
        "id": "DDBVfqapu9UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "detector = cv2.aruco.CharucoDetector(board, detectorParams=parameters)"
      ],
      "metadata": {
        "id": "jXvDqcT7RSIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 - Find the corners on the image\n",
        "We are going to run the first step, which is to find corners in the calibration board. The second step will involve matching these from framet to frame. Becuase we'll reuse it often to compare calibration, let's define a function named **find_corners()**"
      ],
      "metadata": {
        "id": "pJORvlk2R0gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\color{#BB5544}{\\text{TODO: Find the corners for left and right using detector.detectBoard() function. - There is one argument to pass, what is it?}}$\n"
      ],
      "metadata": {
        "id": "xlazG_xewkuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_corners():\n",
        "\n",
        "  # Convert images to grayscale\n",
        "  grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)\n",
        "  grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  frameSize = (grayR.shape[1], grayR.shape[0])\n",
        "\n",
        "  # Detect the Charuco board in both images\n",
        "  c_corners_left, c_ids_left, _, _ = #TODO\n",
        "  retL = len(c_corners_left)\n",
        "  print(\"Detected\", retL, \"corners in left image\")\n",
        "\n",
        "  c_corners_right, c_ids_right, _, _ = #TODO\n",
        "  retR = len(c_corners_right)\n",
        "  print(\"Detected\", retR, \"corners in right image\")\n",
        "\n",
        "  # Copy images to draw on\n",
        "  imgL_with_corners = imgL.copy()\n",
        "  imgR_with_corners = imgR.copy()\n",
        "\n",
        "  # Draw detected corners and IDs on the left image\n",
        "  if c_corners_left is not None and c_ids_left is not None:\n",
        "      cv2.aruco.drawDetectedCornersCharuco(imgL_with_corners, c_corners_left, c_ids_left)\n",
        "\n",
        "\n",
        "  # Draw detected corners and IDs on the right image\n",
        "  if c_corners_right is not None and c_ids_right is not None:\n",
        "      cv2.aruco.drawDetectedCornersCharuco(imgR_with_corners, c_corners_right, c_ids_right)\n",
        "\n",
        "  # Convert images to RGB for plotting\n",
        "  imgL_rgb = cv2.cvtColor(imgL_with_corners, cv2.COLOR_BGR2RGB)\n",
        "  imgR_rgb = cv2.cvtColor(imgR_with_corners, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # Plot images side by side\n",
        "  plt.figure(figsize=(20, 10))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(imgL_rgb)\n",
        "  plt.title('Left Image with Detected Corners and IDs')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(imgR_rgb)\n",
        "  plt.title('Right Image with Detected Corners and IDs')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  objpoints.append(objp)\n",
        "\n",
        "  idsL.append(c_ids_left)\n",
        "  cornersL.append(c_corners_left)\n",
        "\n",
        "  idsR.append(c_ids_right)\n",
        "  cornersR.append(c_corners_right)"
      ],
      "metadata": {
        "id": "HH1IEdM9R3S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run this function for just ONE image.\n",
        "\n",
        "$\\color{#BB5544}{\\text{TODO: Pick an image from the dataset and visualize the output?}}$\n"
      ],
      "metadata": {
        "id": "yELxZ5nzSfJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the left and right images\n",
        "pathImgL = '/content/calib_data/charuco/left/___'#TODO\n",
        "pathImgR = '/content/calib_data/charuco/right/'#TODO\n",
        "imagesLeft.append(pathImgL)\n",
        "imagesRight.append(pathImgR)\n",
        "\n",
        "imgL = cv2.imread(pathImgL)\n",
        "imgR = cv2.imread(pathImgR)\n",
        "\n",
        "find_corners()"
      ],
      "metadata": {
        "id": "9YMZY1WNSJr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool! You can see the patterns in blue! Because we're in Stereo, we did it for the two images, but we could do that for just one camera, the process would be the same!"
      ],
      "metadata": {
        "id": "-BZpHlHYSiwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Match Image Points to Object Points\n",
        "Again, we'll do this in Stereo Mode; but if you want to do that for one image, you can just remove the left or right functions. We are going to use a visualizer helper function to make a cool visualization."
      ],
      "metadata": {
        "id": "2AMay9zNSs1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_correspondences(img, imgPoints, objPoints, ids):\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "    # Convert image to RGB for plotting\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Create a figure with two subplots\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "\n",
        "    # Plot 2D image points with IDs\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.imshow(img_rgb)\n",
        "    for i, pt in enumerate(imgPoints):\n",
        "        x, y = pt.ravel()\n",
        "        ax1.scatter(x, y, c='r', marker='o')\n",
        "        ax1.text(x + 5, y - 5, str(ids[i][0]), color='yellow', fontsize=12)\n",
        "    ax1.set_title('2D Image Points with IDs')\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # Ensure objPoints is a numpy array\n",
        "    objPoints = np.array(objPoints)\n",
        "\n",
        "    # Reshape objPoints if necessary\n",
        "    if objPoints.ndim == 3:\n",
        "        objPoints = objPoints.reshape(-1, 3)\n",
        "    elif objPoints.ndim == 2 and objPoints.shape[1] == 1:\n",
        "        objPoints = np.vstack([pt for pt in objPoints[:, 0]])\n",
        "    elif objPoints.ndim == 1:\n",
        "        objPoints = np.vstack(objPoints)\n",
        "\n",
        "    # Now extract X, Y, Z coordinates\n",
        "    X = objPoints[:, 0]\n",
        "    Y = objPoints[:, 1]\n",
        "    Z = objPoints[:, 2]\n",
        "\n",
        "    # Plot 3D object points with IDs\n",
        "    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
        "    ax2.scatter(X, Y, Z, c='b', marker='^')\n",
        "    for i in range(len(objPoints)):\n",
        "        ax2.text(X[i], Y[i], Z[i], str(ids[i][0]), color='red', fontsize=10)\n",
        "    ax2.set_title('3D Object Points with IDs')\n",
        "    ax2.set_xlabel('X (m)')\n",
        "    ax2.set_ylabel('Y (m)')\n",
        "    ax2.set_zlabel('Z (m)')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7OJsPij4TxqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\color{#BB5544}{\\text{TODO: Match the objects to the images using the board.matchImagePoints(corners, id) function}}$"
      ],
      "metadata": {
        "id": "rZLRZ_zuyZpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_object_images():\n",
        "  c_corners_left, c_ids_left = cornersL[-1], idsL[-1] # the detected corners and ids for the left image\n",
        "  c_corners_right, c_ids_right = cornersR[-1], idsR[-1] # the detected corners and ids for the right image\n",
        "\n",
        "  objPointsL, imgPointsL = #TODO\n",
        "  imgpointsL.append(imgPointsL)\n",
        "  objpointsL.append(objPointsL)\n",
        "\n",
        "  imagesL.append(imgL)\n",
        "\n",
        "  objPointsR, imgPointsR = #TODO\n",
        "  imgpointsR.append(imgPointsR)\n",
        "  objpointsR.append(objPointsR)\n",
        "\n",
        "  imagesR.append(imgR)\n",
        "\n",
        "  # Assuming you have imgL, imgPointsL, objPointsL, and c_ids_left\n",
        "  visualize_correspondences(imgL, imgPointsL, objPointsL, c_ids_left)"
      ],
      "metadata": {
        "id": "swgO-vywTIUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "match_object_images()"
      ],
      "metadata": {
        "id": "nWCXc9zdUH2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wait, how is that possible?** Well, it's because of all the parameters we defined, remember? We know we have a 27 inch screen, we know the size of each corner, we know every possible real-world value; so this function is actually saying \"See this 10 pixel difference? That's 2 cm in reality\"!<p>\n",
        "Realize something: the visualization we ran here is for the left image only. So it's really a mono-calibration so far, done separately on every image.\n"
      ],
      "metadata": {
        "id": "0xQdc-QIUMi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Want to test on other images?\n",
        "Change the filename below, and let's try it again!"
      ],
      "metadata": {
        "id": "C5myWCDLU6R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the left and right images\n",
        "pathImgL = '/content/calib_data/charuco/left/ZED_image_left_21.png'\n",
        "pathImgR = '/content/calib_data/charuco/right/ZED_image_right_21.png'\n",
        "imagesLeft.append(pathImgL)\n",
        "imagesRight.append(pathImgR)\n",
        "\n",
        "imgL = cv2.imread(pathImgL)\n",
        "imgR = cv2.imread(pathImgR)\n",
        "\n",
        "find_corners() #step 1\n",
        "match_object_images() #step2"
      ],
      "metadata": {
        "id": "FeVJnJZOU95y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 — Calibrate!\n",
        "We are now ready for the calibration, which means the generation of the camera's **intrinsic** AND **extrinsic** parameters. Because we have the chance to have two images, we can also do the extrinsic parameters between Image Left and Right later. For now, let's focus on the calibration of each of the cameras..."
      ],
      "metadata": {
        "id": "E0pVCbZzVaQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Let's begin with the intrinsics**"
      ],
      "metadata": {
        "id": "-J6y-v415hQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\color{#BB5544}{\\text{TODO: Use the cv2.CalibrateCamera(object, images, framezsize, None,None) function}}$"
      ],
      "metadata": {
        "id": "dEFfeJq6zmbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calibrate():\n",
        "  print(\"\\nCalibrating left cam\")\n",
        "  retL, cameraMatrixL, distL, rvecsL, tvecsL = #TODO\n",
        "  heightL, widthL, channelsL = imgL.shape\n",
        "  newCameraMatrixL, roi_L = cv2.getOptimalNewCameraMatrix(cameraMatrixL, distL, (widthL, heightL), 1, (widthL, heightL))\n",
        "  print(\"Image size = \", frameSize)\n",
        "  print(\"Reprojection error = \", retL)\n",
        "  print(\"Intrinsic parameter K = \", cameraMatrixL)\n",
        "  print(\"Distortion parameters d = \", distL)\n",
        "\n",
        "  print(\"\\nCalibrating right cam\")\n",
        "  retR, cameraMatrixR, distR, rvecsR, tvecsR = #T0DO\n",
        "  heightR, widthR, channelsR = imgR.shape\n",
        "  newCameraMatrixR, roi_R = cv2.getOptimalNewCameraMatrix(cameraMatrixR, distR, (widthR, heightR), 1, (widthR, heightR))\n",
        "  print(\"Image size = \", frameSize)\n",
        "  print(\"Reprojection error = \", retR)\n",
        "  print(\"Intrinsic parameter K = \", cameraMatrixR)\n",
        "  print(\"Distortion parameters d = \", distR)\n",
        "\n",
        "  return newCameraMatrixL, distL, rvecsL, tvecsL, newCameraMatrixR, distR, rvecsR, tvecsR"
      ],
      "metadata": {
        "id": "pwqTGKs5Vk_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About the function:**\n",
        "We are sending the left image points (2D) and the object points (3D) to the function. We then do the same for right. The function is using this on all corners of the image to find the parameters. <p>\n",
        "\n",
        "Then, it returns:\n",
        "* **retL / retR**: The reprojection error, a measure of how well the points project back into the image.\n",
        "* **cameraMatrixL / cameraMatrixR**: The intrinsic camera matrix for the left and right cameras, respectively.\n",
        "* **distL / distR**: The distortion coefficients for the left and right cameras."
      ],
      "metadata": {
        "id": "ZqmDCqukVxPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cameraMatrixL, distL, rvecsL, tvecsL, cameraMatrixR, distR, rvecsR, tvecsR = calibrate()"
      ],
      "metadata": {
        "id": "J2i4rtVpWQ4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So What Did the Calibration Give us? 3 things:**<p>\n",
        "\n",
        "1. **The Intrinsic Parameters**, encapsulated in a matrix **K**. It is of shape 3x3 and encapsulates the values of the camera. Skew (s, usually 0 unless axis are not perpendicular), Offset (cx, cy), Focal Length (fx, fy). Its general form is:\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "\n",
        "2. **The Distortion Coefficients** (distL, distR); which model the lens distortion of a camera. They are crucial for correcting images to ensure accurate measurements and representations. A few types of distortions are possible, **radial** (causing straight lines to appear curved, resulting in barrel or pincushion distortion), **barrel** (causes images to bulge outward at the center, making straight lines appear to curve inward toward the center, resembling the shape of a barrel), **pincushion** (causes images to pinch inward at the center, making straight lines appear to curve outward away from the center, resembling the shape of a pincushion), or even **tengential** (which occurs when the lens and image sensor are not perfectly parallel, causing asymmetrical distortion). The distortion coefficients (distL or distR) returned by the camera calibration are:[k1, k2, p1, p2, k3]\n",
        "\n",
        "3. **The Reprojection Error**:\n",
        "The reprojection error is a measure of how the calibration performed. This happens at tehe end of calibration, to validate our parameters, we use them to project the 3D points back to the image, and calculate the difference between the observed and projected 2D points. Typically:\n",
        "* A Low Reprojection Error Indicates that the estimated camera parameters accurately model the imaging process. The projected points closely match the observed points.\n",
        "* A High Reprojection Error suggests inaccuracies in calibration. Possible causes include poor feature detection, insufficient calibration images, or unmodeled lens distortions."
      ],
      "metadata": {
        "id": "4UlrvB3AYgia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **What about t and R? Aren't they given?**\n",
        "Yes, if you notice correctly, the function is like this:\n",
        "`retL, cameraMatrixL, distL, rvecsL, tvecsL = cv2.calibrateCamera(objpointsL, imgpointsL, frameSize, None, None) # calibrate left`\n",
        "Which gives us rvecsL and rvecsL. Let's print these:\n",
        "\n"
      ],
      "metadata": {
        "id": "ms67l3Ep71xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rvecsL) #rotation vectors"
      ],
      "metadata": {
        "id": "FJW0UWyL8CAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tvecsL) #translation vectors"
      ],
      "metadata": {
        "id": "pPpdqyuY8E3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These represent each camera's pose (rotation and translation) **relative** to the charuco board. If you compute it for 20 images, you're gonna have 20 different parameters. We don't want this: we want only ONE extrinsic parameter for the camera, representing the camera's relationship with the world. We want **absolute**."
      ],
      "metadata": {
        "id": "UgmL2yry8LC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Now, let's see the Extrinsic parameters**\n",
        "In theory, the original calibration function could immediately return the extrinsics. For the sake of this project, we'll use the function **solvePnP** from OpenCV: `cv2.solvePnP(objPoints, imgPoints, K, d)` to  find the extrinsics.\n",
        "\n",
        "**What does it do?** <p>\n",
        "This function estimates the **pose** (position and orientation) of a calibrated camera with respect to a known object or scene. It computes the **rotation and translation vectors** (`rvec`, `tvec`) that map 3D object points to their corresponding 2D image points, given the camera's intrinsic parameters and distortion coefficients.\n",
        "\n",
        "\n",
        "Parameters:\n",
        "- **objPoints** and **imgPoints**\n",
        "- **K** (intrinsics)  \n",
        "- **d** (distortion coefficients)\n",
        "\n",
        "Return Values:\n",
        "- **retval**: Boolean flag indicating success (`True`) or failure (`False`) of the pose estimation.\n",
        "- **rvec, tvec**: Output rotation and translation vectors.\n",
        "  "
      ],
      "metadata": {
        "id": "RHFblrYnWnMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In order to make it REALLY COOL**, we are going to define a few function to visualize the cameras in 3D and their poses.\n",
        "\n",
        "For this, we need to obtain the pose in the camera-to-world.\n",
        "We can use this code for that:\n",
        "```python\n",
        "R_w2c = cv2.Rodrigues(p_rvec)[0]  # Convert the rotation vector to a rotation matrix (world-to-camera)\n",
        "R_c2w = np.linalg.inv(R_w2c)  # Invert the rotation matrix to get camera-to-world rotation\n",
        "t_c2w = -R_c2w.dot(p_tvec).reshape((1,3))  # Calculate camera-to-world translation\n",
        "```\n",
        "The result, R_c2w and t_c2w, can be used to plot the camera pose in the world coordinate system.\n",
        "**"
      ],
      "metadata": {
        "id": "e5y1O9V2Xsj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotCamera(ax, R, t, *, color=None, scale=1, width=2, height=1.5, focal_length=3):\n",
        "\n",
        "    t = t.reshape((3, 1))\n",
        "\n",
        "    w = width\n",
        "    h = height\n",
        "    f = focal_length\n",
        "\n",
        "    # focus, br, tr, tl, bl, triangle\n",
        "    ps_c = np.array(([0,0,0], [w/2,h/2,f], [w/2,-h/2,f], [-w/2,-h/2,f], [-w/2,h/2,f], [0,-h/2-h/4,f]))\n",
        "    ps_w = (scale * R @ ps_c.T + t).T\n",
        "\n",
        "    L01 = np.array([ps_w[0], ps_w[1]])\n",
        "    L02 = np.array([ps_w[0], ps_w[2]])\n",
        "    L03 = np.array([ps_w[0], ps_w[3]])\n",
        "    L04 = np.array([ps_w[0], ps_w[4]])\n",
        "    L1234 = np.array([ps_w[1], ps_w[2], ps_w[3], ps_w[4], ps_w[1]])\n",
        "    L253 = np.array([ps_w[2], ps_w[5], ps_w[3]])\n",
        "\n",
        "    p = ax.plot(L01[:,0], L01[:,1], L01[:,2], \"-\", color=color)\n",
        "    if color is None:\n",
        "        color = p[-1].get_color()\n",
        "    ax.plot(L02[:,0], L02[:,1], L02[:,2], \"-\", color=color)\n",
        "    ax.plot(L03[:,0], L03[:,1], L03[:,2], \"-\", color=color)\n",
        "    ax.plot(L04[:,0], L04[:,1], L04[:,2], \"-\", color=color)\n",
        "    ax.plot(L1234[:,0], L1234[:,1], L1234[:,2], \"-\", color=color)\n",
        "    ax.plot(L253[:,0], L253[:,1], L253[:,2], \"-\", color=color)"
      ],
      "metadata": {
        "id": "hxeImfGbYFSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next the high level function that computes the extrinsics:"
      ],
      "metadata": {
        "id": "3s7dS70A697n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_camera_poses(imgs_all_corners, K, d, all_obj_pts, all_img_pts, all_ids, all_corners):\n",
        "    # plotCamera() config\n",
        "    plot_mode   = 1    # 0: fixed camera / moving chessboard,  1: fixed chessboard, moving camera\n",
        "    plot_range  = 0.5 # target volume [-plot_range:plot_range]\n",
        "    camera_size = 0.02  # size of the camera in plot\n",
        "\n",
        "    for i, objPoints, imgPoints, c_ids, c_corners in zip(imgs_all_corners, all_obj_pts, all_img_pts, all_ids, all_corners):\n",
        "        frame = cv2.imread(i)\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        ret, p_rvec, p_tvec = cv2.solvePnP(objPoints, imgPoints, K, d)\n",
        "\n",
        "        if p_rvec is None or p_tvec is None:\n",
        "            continue\n",
        "        if np.isnan(p_rvec).any() or np.isnan(p_tvec).any():\n",
        "            continue\n",
        "        cv2.drawFrameAxes(frame,\n",
        "                        K,\n",
        "                        d,\n",
        "                        p_rvec,\n",
        "                        p_tvec,\n",
        "                        0.1)\n",
        "        cv2.aruco.drawDetectedCornersCharuco(frame, c_corners, c_ids)\n",
        "\n",
        "        r_matrix = cv2.Rodrigues(p_rvec)[0]\n",
        "        print('Translation : {0}'.format(p_tvec))\n",
        "        print('Rotation Vector : {0}'.format(p_rvec))\n",
        "        print('Rotation Matrix : {0}'.format(r_matrix))\n",
        "        print('Distance from camera: {0} m'.format(np.linalg.norm(p_tvec)))\n",
        "\n",
        "        # show sample image\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(frame_rgb)\n",
        "        plt.show()\n",
        "\n",
        "        # Plot\n",
        "        fig_ex = plt.figure()\n",
        "\n",
        "        ax_ex = Axes3D(fig_ex, auto_add_to_figure=False)\n",
        "        ax_ex.view_init(elev=None, azim=40, vertical_axis='y')\n",
        "        fig_ex.add_axes(ax_ex)\n",
        "\n",
        "        ax_ex.set_xlim(-plot_range, plot_range)\n",
        "        ax_ex.set_ylim(-plot_range, plot_range)\n",
        "        ax_ex.set_zlim(-plot_range, plot_range)\n",
        "\n",
        "        R_w2c = cv2.Rodrigues(p_rvec)[0]\n",
        "        R_c2w = np.linalg.inv(R_w2c)\n",
        "        t_c2w = -R_c2w.dot(p_tvec).reshape((1,3))\n",
        "\n",
        "        plotCamera(ax_ex, R_c2w, t_c2w, color=\"b\", scale=camera_size)\n",
        "        print(objPoints.shape)\n",
        "        ax_ex.plot(objPoints[:,0,0], objPoints[:,0,1], objPoints[:,0,2], \".\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "wFv0euqEXv_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's call it:"
      ],
      "metadata": {
        "id": "_bFk21Q_6_u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_camera_poses(imagesLeft, cameraMatrixL, distL, objpointsL, imgpointsL, idsL, cornersL)"
      ],
      "metadata": {
        "id": "VP1-kN14X3g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 — Visualize Distortions & Corrections\n",
        "The Zed 2 camera has very little distortion, so you will have to look very carefully no notice the difference between the distorted and undistorted images.Let's take a look anyway."
      ],
      "metadata": {
        "id": "LALPJS-x0bVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Computing undistortion maps...\")\n",
        "undistort_map1 = cv2.initUndistortRectifyMap(cameraMatrixL,\n",
        "                                            distL,\n",
        "                                            np.eye(3),\n",
        "                                            cameraMatrixL,\n",
        "                                            frameSize,\n",
        "                                            cv2.CV_32F)\n",
        "\n",
        "# Load a stereo image pair\n",
        "img_fn = '/content/calib_data/charuco/left/ZED_image_left_21.png'\n",
        "img1_left_raw = cv2.imread(img_fn)\n",
        "\n",
        "# show sample image\n",
        "frame_rgb = cv2.cvtColor(img1_left_raw, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(frame_rgb)\n",
        "plt.show()\n",
        "\n",
        "# Undistort images - undistortion example\n",
        "print(\"Undistorting image\")\n",
        "img1_left_undist = cv2.remap(img1_left_raw, *undistort_map1, cv2.INTER_LANCZOS4)\n",
        "\n",
        "# show sample image\n",
        "frame_rgb = cv2.cvtColor(img1_left_undist, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(frame_rgb)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0xa_GOYU7Gy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 — Stereo Calibration & Main Function\n",
        "Okay, we now have a way to calibrate, should we build a \"main\" function we can use that encapsulates everything? You'll see that we can do it, and we can also run Stereo Calibration!\n",
        "\n",
        "#### How does Stereo Calibration work?\n",
        "The principle is quite simple: you're going to use both of the parameter you found and try to compute parameter from camera Left to camera Right. The goal here is to find a rotation and translation matrix that tell you which movement you need to go from Camera Left to Right.<p>\n",
        "The function used is `cv2.stereoCalibrate()`\n"
      ],
      "metadata": {
        "id": "YYNUVn3A-l9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\color{#BB5544}{\\text{TODO: Repeat the process with the 3 key functions to run the calibration.}}$"
      ],
      "metadata": {
        "id": "GcjOU9ke0K3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calibration(strict = True):\n",
        "  '''Termination criteria\n",
        "  Description: These are termination criteria used in iterative algorithms to define when to stop.\n",
        "  Parameters:\n",
        "  cv2.TERM_CRITERIA_EPS: Stops when the change in parameter values is less than the specified threshold.\n",
        "  cv2.TERM_CRITERIA_MAX_ITER: Stops after a specified maximum number of iterations.\n",
        "  30: The maximum number of iterations for the optimization process.\n",
        "  0.001: The minimum accuracy to stop the optimization process.\n",
        "  Usage in Code: This termination criterion is used for subpixel corner detection, controlling the precision and iteration limits.\n",
        "  '''\n",
        "  criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "  print(charuco_object_points.shape)\n",
        "\n",
        "  # Assuming you have the object points from the Charuco board\n",
        "  objp = np.array(charuco_object_points)  # Shape: (78, 3)\n",
        "\n",
        "  # Arrays to store object points and image points from all the images.\n",
        "  objpoints = [] # 3d point in real world space\n",
        "  imgpointsL = [] # 2d points in image plane.\n",
        "  imgpointsR = [] # 2d points in image plane.\n",
        "  objpointsL = [] # 3d point in real world space\n",
        "  objpointsR = [] # 3d point in real world space\n",
        "\n",
        "  idsL = []\n",
        "  cornersL = []\n",
        "  imagesL = []\n",
        "  idsR = []\n",
        "  cornersR = []\n",
        "  imagesR = []\n",
        "\n",
        "\n",
        "  imagesLeft = sorted(glob(CALIBRATION_IMAGES_PATH_LEFT))\n",
        "  imagesRight = sorted(glob(CALIBRATION_IMAGES_PATH_RIGHT))\n",
        "\n",
        "  parameters =  cv2.aruco.DetectorParameters()\n",
        "  parameters.cornerRefinementMethod = cv2.aruco.CORNER_REFINE_CONTOUR\n",
        "  '''\n",
        "  Description: This creates a Charuco detector object that is used to detect both ArUco markers and chessboard corners.\n",
        "  Parameters:\n",
        "  board: The Charuco board used for calibration.\n",
        "  detectorParams: The detection parameters configured earlier, such as corner refinement.\n",
        "  Usage in Code: This detector helps in finding corners and IDs of the ArUco markers on the Charuco board from the calibration images.\n",
        "  '''\n",
        "  detector = #TODO: Initialize a Detector\n",
        "\n",
        "  # count = 0\n",
        "  for imgLeft, imgRight in zip(imagesLeft, imagesRight):\n",
        "\n",
        "      print(imgLeft, imgRight)\n",
        "      imgL = cv2.imread(imgLeft)\n",
        "      imgR = cv2.imread(imgRight)\n",
        "      grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)\n",
        "      grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "      c_corners_left, c_ids_left, corners, ids = #TODO: Detect the board\n",
        "      if c_corners_left is None:\n",
        "          continue\n",
        "      retL = len(c_corners_left)\n",
        "\n",
        "      c_corners_right, c_ids_right, corners, ids = #TODO: Detect the Board\n",
        "      if c_corners_right is None:\n",
        "          continue\n",
        "      retR = len(c_corners_right)\n",
        "\n",
        "      frameSize = (grayR.shape[1], grayR.shape[0])\n",
        "\n",
        "\n",
        "      if (not strict and retL >= 40 and retR >= 40 and retL == retR)  or (retL == retR and retR == 78):\n",
        "          print(\"Found\", retL, retR,  \"corners\")\n",
        "          objpoints.append(objp)\n",
        "\n",
        "          '''\n",
        "          Description: Matches the detected 2D points (corners) from the Charuco board in the image with the corresponding 3D points (known positions) on the Charuco board.\n",
        "          Parameters:\n",
        "          c_corners_left / c_corners_right:\n",
        "          The detected 2D coordinates (corners) of the Charuco board in the left and right camera images, respectively.\n",
        "          These points are detected by the Charuco board detector and represent the positions of the Charuco corners in the image.\n",
        "          c_ids_left / c_ids_right:\n",
        "          The IDs of the detected ArUco markers associated with the corners.\n",
        "          These IDs help identify the specific markers detected on the Charuco board, allowing for accurate matching between the detected points and the known 3D positions on the board.\n",
        "          Return Values:\n",
        "          objPointsL / objPointsR:\n",
        "          The 3D object points (real-world coordinates) corresponding to the detected corners in the left and right camera images, respectively.\n",
        "          These points are known, fixed positions on the Charuco board in real-world units (like meters).\n",
        "          imgPointsL / imgPointsR:\n",
        "          The 2D image points (detected corner coordinates) from the left and right camera images, respectively.\n",
        "          These points are the pixel locations in the images where the Charuco corners were detected, which are mapped to the corresponding 3D object points.\n",
        "          Usage in Code: This function links the 2D points detected in the left and right images with their corresponding 3D points on the Charuco board.\n",
        "          These matched points (2D image points and 3D object points) are essential for the camera calibration process, as they provide the data needed to\n",
        "          compute the camera's intrinsic parameters and the stereo camera's relative position and orientation.\n",
        "          '''\n",
        "          objPointsL, imgPointsL = #TODO: Match Image-Object Points\n",
        "          imgpointsL.append(imgPointsL)\n",
        "          objpointsL.append(objPointsL)\n",
        "          idsL.append(c_ids_left)\n",
        "          cornersL.append(c_corners_left)\n",
        "          imagesL.append(imgLeft)\n",
        "\n",
        "          objPointsR, imgPointsR = #TODO: Match Image-Object Points\n",
        "          imgpointsR.append(imgPointsR)\n",
        "          objpointsR.append(objPointsR)\n",
        "          idsR.append(c_ids_right)\n",
        "          cornersR.append(c_corners_right)\n",
        "          imagesR.append(imgRight)\n",
        "\n",
        "          # count += 1\n",
        "          # if count > 5:\n",
        "          #   break\n",
        "\n",
        "  ############# CALIBRATION #######################################################\n",
        "  '''\n",
        "  Description: Calibrates the camera by estimating its intrinsic parameters and distortion coefficients.\n",
        "  Parameters:\n",
        "  objpointsL / objpointsR: List of 3D object points in the real world for the left and right cameras.\n",
        "  imgpointsL / imgpointsR: List of 2D image points for the left and right cameras, detected in the images.\n",
        "  frameSize: The size of the image in pixels.\n",
        "  None: No initial intrinsic parameters are passed.\n",
        "  Return Values:\n",
        "  retL / retR: The reprojection error, a measure of how well the points project back into the image.\n",
        "  cameraMatrixL / cameraMatrixR: The intrinsic camera matrix for the left and right cameras, respectively.\n",
        "  distL / distR: The distortion coefficients for the left and right cameras.\n",
        "  Usage in Code: Calibrates each camera separately, estimating intrinsic parameters such as focal length and optical center, and correcting for lens distortion.\n",
        "  '''\n",
        "  print(\"\\nCalibrating left cam\")\n",
        "  retL, cameraMatrixL, distL, rvecsL, tvecsL = #TODO: Calibrate the Camera\n",
        "  heightL, widthL, channelsL = imgL.shape\n",
        "  newCameraMatrixL, roi_L = cv2.getOptimalNewCameraMatrix(cameraMatrixL, distL, (widthL, heightL), 1, (widthL, heightL))\n",
        "  print(\"Image size = \", frameSize)\n",
        "  print(\"Reprojection error = \", retL)\n",
        "  print(\"Intrinsic parameter K = \", cameraMatrixL)\n",
        "  print(\"Distortion parameters d = \", distL)\n",
        "\n",
        "  print(\"\\nCalibrating right cam\")\n",
        "  retR, cameraMatrixR, distR, rvecsR, tvecsR = #TODO: Calibrate the Camera\n",
        "  heightR, widthR, channelsR = imgR.shape\n",
        "  newCameraMatrixR, roi_R = cv2.getOptimalNewCameraMatrix(cameraMatrixR, distR, (widthR, heightR), 1, (widthR, heightR))\n",
        "  print(\"Image size = \", frameSize)\n",
        "  print(\"Reprojection error = \", retR)\n",
        "  print(\"Intrinsic parameter K = \", cameraMatrixR)\n",
        "  print(\"Distortion parameters d = \", distR)\n",
        "\n",
        "  ######### Stereo Vision Calibration #############################################\n",
        "  flags = 0\n",
        "  flags = cv2.CALIB_FIX_INTRINSIC\n",
        "\n",
        "  criteria_stereo= (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.0001)\n",
        "\n",
        "  '''\n",
        "  Description: Performs stereo calibration to determine the relationship between two cameras, including extrinsic parameters like rotation (R) and translation (T).\n",
        "  Parameters:\n",
        "  objpointsL: 3D object points from the left camera.\n",
        "  imgpointsL: 2D image points from the left camera.\n",
        "  imgpointsR: 2D image points from the right camera.\n",
        "  cameraMatrixL / cameraMatrixR: The intrinsic matrices for the left and right cameras.\n",
        "  distL / distR: Distortion coefficients for the left and right cameras.\n",
        "  frameSize: The image size.\n",
        "  criteria_stereo: Termination criteria for the optimization process.\n",
        "  flags: This specifies how the stereo calibration should be done. In this case, cv2.CALIB_FIX_INTRINSIC is used to keep the intrinsic parameters fixed during stereo calibration.\n",
        "  Return Values:\n",
        "  retStereo: The reprojection error for the stereo pair.\n",
        "  rot: The rotation matrix describing the orientation difference between the two cameras.\n",
        "  trans: The translation vector between the two cameras.\n",
        "  essentialMatrix: Describes the epipolar geometry between the cameras.\n",
        "  fundamentalMatrix: Describes the relationship between corresponding points in both images.\n",
        "  Usage in Code: This function calibrates the stereo setup, providing the extrinsic parameters (rotation and translation) that describe the spatial relationship between the two cameras.\n",
        "  '''\n",
        "  print(\"\\nCalibrating stereo\")\n",
        "  retStereo, newCameraMatrixL, distL, newCameraMatrixR, distR, rot, trans, essentialMatrix, fundamentalMatrix = #TODO: Calibrate the Camera in Stereo Mode\n",
        "  # ret_stereo, K_left_stereo, d_left_stereo, K_right_stereo, d_right_streo, R, T, E, F\n",
        "  print('\\nstereo vision done')\n",
        "  print(\"Reprojection error = \", retStereo)\n",
        "  print(\"INTRINSIC PARAMETERS:\")\n",
        "  print(\" LEFT CAMERA:\")\n",
        "  print(\" Intrinsic parameter K = \", newCameraMatrixL)\n",
        "  print(\" Distortion parameters d = \", distL)\n",
        "  print(\" RIGHT CAMERA:\")\n",
        "  print(\" Intrinsic parameter K = \", newCameraMatrixR)\n",
        "  print(\" Distortion parameters d = \", distR)\n",
        "  print(\"EXTRINSIC PARAMETERS - LEFT TO RIGHT:\")\n",
        "  print(\" Rotation matrix = \", rot)\n",
        "  print(\" Translation vector = \", trans)\n",
        "  print(\"EPIPOLAR GEOMETRY\")\n",
        "  print(\" Essential matrix = \", essentialMatrix)\n",
        "  print(\" Fundamental matrix = \", fundamentalMatrix)\n",
        "\n",
        "  return frameSize, imagesL, objpointsL, imgpointsL, idsL, cornersL, newCameraMatrixL, distL, rvecsL, tvecsL, imagesR, objpointsR, imgpointsR, idsR, cornersR, newCameraMatrixR, distR, rvecsR, tvecsR, rot, trans, fundamentalMatrix, essentialMatrix\n"
      ],
      "metadata": {
        "id": "S_M-8B0x_F5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's run a calibration with the good images!"
      ],
      "metadata": {
        "id": "3ccBzRWNz-Bd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9xXG0e606rm",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Paths to the captured frames (should be in synch) (stereoLeft and stereoRight)\n",
        "CALIBRATION_IMAGES_PATH_LEFT = '/content/calib_data/charuco/left/*.png'\n",
        "CALIBRATION_IMAGES_PATH_RIGHT = '/content/calib_data/charuco/right/*.png'\n",
        "\n",
        "frameSize, imagesL, objpointsL, imgpointsL, idsL, cornersL, cameraMatrixL, distL, rvecsL, tvecsL, imagesR, objpointsR, imgpointsR, idsR, cornersR, cameraMatrixR, distR, rvecsR, tvecsR, rot, trans, fundamentalMatrix, essentialMatrix = calibration(strict = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blV-5EXhBr2L"
      },
      "source": [
        "## Step 4 — Show the Camera Poses\n",
        "This step is really for fun, but it also looks really good.\n",
        "### Individual Poses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T9CoFi74v41"
      },
      "outputs": [],
      "source": [
        "numImgs = len(imagesL)\n",
        "plot_camera_poses(imagesL[:numImgs], cameraMatrixL[:numImgs], distL[:numImgs], objpointsL[:numImgs], imgpointsL[:numImgs], idsL[:numImgs], cornersL[:numImgs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "723mE7fJBxnE"
      },
      "source": [
        "### All Poses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_camera_all_poses(objPoints, rvec, tvec):\n",
        "\n",
        "    # plotCamera() config\n",
        "    plot_range  = 0.5\n",
        "    camera_size = 0.02\n",
        "\n",
        "    fig_in = plt.figure()\n",
        "    ax_in = Axes3D(fig_in, auto_add_to_figure=False)\n",
        "    fig_in.add_axes(ax_in)\n",
        "\n",
        "    ax_in.set_xlim(-plot_range, plot_range)\n",
        "    ax_in.set_ylim(-plot_range, plot_range)\n",
        "    ax_in.set_zlim(-plot_range, plot_range)\n",
        "    ax_in.view_init(elev=None, azim=40, vertical_axis='y')\n",
        "\n",
        "    for i_ex in range(len(rvec)):\n",
        "        R_c2w = np.linalg.inv(cv2.Rodrigues(rvec[i_ex])[0])\n",
        "        t_c2w = -R_c2w.dot(tvec[i_ex]).reshape((1,3))\n",
        "\n",
        "        plotCamera(ax_in, R_c2w, t_c2w, color=\"b\", scale=camera_size)\n",
        "\n",
        "    ax_in.plot(objPoints[:,0,0], objPoints[:,0,1], objPoints[:,0,2], \".\")\n"
      ],
      "metadata": {
        "id": "TN7v1wUfBLm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4BQg3vg-kxv"
      },
      "outputs": [],
      "source": [
        "plot_camera_all_poses(objpointsL[0], rvecsL, tvecsL)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is cool, admit it. We are done with Part 2 on calibration!\n",
        "\n",
        "<p>\n",
        "\n",
        "What now?\n",
        "\n",
        "We are officially done with Module 1, and we are going to jump to Module 2! But before this, I would like to take 3 minutes and have you run the next cells. Even if you don't understand them yet (as this will be the point of Module 2), I'd like you to see the impact of a good and a bad calibration on the 3D Reconstruction.<p>\n",
        "\n",
        "* **ASSIGNMENT 1**: Naively run the next cells to visualize the output of a good versus a bad calibration\n",
        "* **ASSIGNMENT 2**: Visit Module 2 and learn about the concepts of epipolar geometry, depth maps, and disparity.\n",
        "* **ASSIGNMENT 3**: Go back to this part and run the cells, this time understanding them."
      ],
      "metadata": {
        "id": "xmrvUjMIBPeJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_t7lAo8Hu6V"
      },
      "source": [
        "# 3. Epipolar Geometry & 3D Reconstruction\n",
        "\n",
        "Epipolar geometry describes the geometric relationship between two views of the same scene from different perspectives (e.g., from a stereo camera setup). It is fundamental for understanding how corresponding points in one image (from the left camera) relate to points in the other image (from the right camera).\n",
        "\n",
        "In stereo vision, corresponding points in the left and right images must lie on epipolar lines. The goal of epipolar geometry is to find these corresponding points and epipolar lines, which helps in tasks like depth estimation and 3D reconstruction.\n",
        "\n",
        "Process:\n",
        "- We detect keypoints (or corners) in both left and right images using one of two methods.\n",
        "- We use these keypoints to compute epipolar lines in the other image.\n",
        "- We draw the corresponding points in one image and the computed epipolar lines in the other image to visualize the relationship between them.\n",
        "\n",
        "Notes:\n",
        "- Rectification: The key part of stereo vision preprocessing, ensuring that corresponding points lie on the same horizontal line in both images.\n",
        "- Epipolar Geometry: Visualized to show how points in one image correspond to lines in the other image.\n",
        "- Calibration Data: The fundamental matrix and rectification transforms (R1, R2, P1, P2) come from the stereo calibration process and are crucial for accurate epipolar line computation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_epipolar_lines(img1, img2, lines1, pts2):\n",
        "    ''' img1 - image on which we draw the epilines for the points in img2\n",
        "        lines - corresponding epilines '''\n",
        "    r, c, _ = img1.shape\n",
        "    for r, pt2 in zip(lines1, pts2):\n",
        "        color = tuple(np.random.randint(0, 255, 3).tolist())\n",
        "        x0, y0 = map(int, [0, -r[2] / r[1]])\n",
        "        x1, y1 = map(int, [c, -(r[2] + r[0] * c) / r[1]])\n",
        "        img1 = cv2.line(img1, (x0, y0), (x1, y1), color, 10)\n",
        "        img2 = cv2.circle(img2, (int(pt2[0]), int(pt2[1])), 15, color, -1)\n",
        "    return img1, img2\n",
        "\n",
        "\n",
        "def show_epipolar_geometry(pathImgL, pathImgR, uml = None, umr = None, method = 1):\n",
        "\n",
        "  imgL = cv2.imread(pathImgL)\n",
        "  imgR = cv2.imread(pathImgR)\n",
        "\n",
        "  # If defined, we will undistort the images\n",
        "  if uml and umr:\n",
        "    # Undistort images\n",
        "    print(\"Undistorting image\")\n",
        "\n",
        "    '''\n",
        "    Description: This function applies the undistortion and rectification maps to the original images,\n",
        "    effectively correcting lens distortion and aligning the images horizontally.\n",
        "    Parameters:\n",
        "    imgL / imgR: The original left and right images.\n",
        "    undistort_map_left / undistort_map_right: The undistortion and rectification maps computed earlier.\n",
        "    cv2.INTER_LANCZOS4: An interpolation method used for remapping, known for its high-quality results.\n",
        "    Return Values:\n",
        "    imgUndistL / imgUndistR: The rectified (undistorted) images.\n",
        "    '''\n",
        "    imgUndistL = cv2.remap(imgL, *uml, cv2.INTER_LANCZOS4)\n",
        "    imgUndistR = cv2.remap(imgR, *umr, cv2.INTER_LANCZOS4)\n",
        "  else:\n",
        "    imgUndistL = imgL\n",
        "    imgUndistR = imgR\n",
        "\n",
        "  # Method 1: Using SIFT to get keypoints\n",
        "  if method == 1:\n",
        "    '''\n",
        "    Description: Creates a SIFT (Scale-Invariant Feature Transform) detector used to detect keypoints and compute descriptors for each keypoint.\n",
        "    Usage in Code: Used to detect feature points (keypoints) in the left and right images for epipolar geometry calculations.\n",
        "    '''\n",
        "    # Find the keypoints and descriptors with SIFT\n",
        "    sift = cv2.SIFT_create()\n",
        "    kp1, des1 = sift.detectAndCompute(imgUndistL, None)\n",
        "    kp2, des2 = sift.detectAndCompute(imgUndistR, None)\n",
        "\n",
        "    '''\n",
        "    Description: Uses the FLANN (Fast Library for Approximate Nearest Neighbors) algorithm to match feature descriptors between the two images.\n",
        "    Parameters:\n",
        "    index_params: Specifies the algorithm and number of trees to be used for matching.\n",
        "    search_params: Specifies the number of times to check each candidate match.\n",
        "    Return Values:\n",
        "    Matches between the keypoints in the left and right images.\n",
        "    Usage in Code: Matches keypoints from the left and right images based on their feature descriptors.\n",
        "    '''\n",
        "    # Use FLANN-based matcher to find matches\n",
        "    FLANN_INDEX_KDTREE = 1\n",
        "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
        "    search_params = dict(checks = 50)\n",
        "\n",
        "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "    matches = flann.knnMatch(des1, des2, k=2)\n",
        "\n",
        "    # Filter matches using the Lowe's ratio test\n",
        "    good_matches = []\n",
        "    ptsL = []\n",
        "    ptsR = []\n",
        "\n",
        "    for m, n in matches:\n",
        "        if m.distance < 0.7 * n.distance:\n",
        "            good_matches.append(m)\n",
        "            ptsL.append(kp1[m.queryIdx].pt)\n",
        "            ptsR.append(kp2[m.trainIdx].pt)\n",
        "\n",
        "    # Sort the matches by distance (quality of the match)\n",
        "    good_matches = sorted(good_matches, key=lambda x: x.distance)\n",
        "\n",
        "    # Choose the top n matches\n",
        "    n = 10  # Set this to the number of top matches you want\n",
        "    ptsL = np.int32([kp1[m.queryIdx].pt for m in good_matches[:n]])\n",
        "    ptsR = np.int32([kp2[m.trainIdx].pt for m in good_matches[:n]])\n",
        "\n",
        "  # Method 2: Using Good Features to Track to get keypoints\n",
        "  else:\n",
        "    '''\n",
        "    Description: Detects good features (keypoints) in an image based on the Harris corner detection algorithm.\n",
        "    Parameters:\n",
        "    Grayscale image: The input image in grayscale.\n",
        "    maxCorners: Maximum number of keypoints to detect.\n",
        "    qualityLevel: Minimum accepted quality of corners.\n",
        "    minDistance: Minimum distance between detected keypoints.\n",
        "    Return Values:\n",
        "    The coordinates of the detected keypoints.\n",
        "    Usage in Code: Used as an alternative method for detecting keypoints if SIFT is not used.\n",
        "    '''\n",
        "    # Detect keypoints in the rectified left image\n",
        "    ptsL = cv2.goodFeaturesToTrack(cv2.cvtColor(imgUndistL, cv2.COLOR_BGR2GRAY), maxCorners=10, qualityLevel=0.01, minDistance=100)\n",
        "    ptsL = np.int32(ptsL).reshape(-1, 2)\n",
        "\n",
        "    # Detect keypoints in the rectified right image\n",
        "    ptsR = cv2.goodFeaturesToTrack(cv2.cvtColor(imgUndistR, cv2.COLOR_BGR2GRAY), maxCorners=10, qualityLevel=0.01, minDistance=100)\n",
        "    ptsR = np.int32(ptsR).reshape(-1, 2)\n",
        "\n",
        "  '''\n",
        "  Description: Computes the epipolar lines for a set of points in one image, mapping them to the corresponding epipolar lines in the other\n",
        "  image based on the fundamental matrix.\n",
        "  Parameters:\n",
        "  ptsL / ptsR: Points from the left or right image.\n",
        "  1 or 2: Specifies whether the points are from the left (1) or right (2) image.\n",
        "  fundamentalMatrix: The fundamental matrix, which describes the epipolar geometry between the two cameras.\n",
        "  Return Values:\n",
        "  linesR / linesL: The epipolar lines corresponding to the points in the opposite image.\n",
        "  Usage in Code: Computes the epipolar lines based on the matched points from the stereo images.\n",
        "  '''\n",
        "  # Compute epilines corresponding to points in the left image, map to the right image\n",
        "  linesR = cv2.computeCorrespondEpilines(ptsL.reshape(-1, 1, 2), 1, fundamentalMatrix)\n",
        "  linesR = linesR.reshape(-1, 3)\n",
        "\n",
        "  '''\n",
        "  Description: Draws epipolar lines on one image and the corresponding points on another image.\n",
        "  These lines represent the possible locations of corresponding points in stereo images.\n",
        "  Parameters:\n",
        "  img1: Image on which to draw the epipolar lines.\n",
        "  img2: Image on which to draw the corresponding points.\n",
        "  lines1: Epipolar lines computed from the fundamental matrix.\n",
        "  pts2: Points from the other image corresponding to the epipolar lines.\n",
        "  Return Values:\n",
        "  img1: The first image with epipolar lines drawn.\n",
        "  img2: The second image with corresponding points marked.\n",
        "  Usage in Code: This function is called to visualize the epipolar lines and corresponding points after computing them from the fundamental matrix.\n",
        "  '''\n",
        "  # Draw the epipolar lines on the right image and the points on the left image\n",
        "  imgR_with_lines, imgL_with_points = draw_epipolar_lines(imgUndistR.copy(), imgUndistL.copy(), linesR, ptsL)\n",
        "  plt.figure(figsize=(20, 10))\n",
        "\n",
        "  # Plot the images with epipolar lines\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(cv2.cvtColor(imgL_with_points, cv2.COLOR_BGR2RGB))\n",
        "  plt.title('Points in left image')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(cv2.cvtColor(imgR_with_lines, cv2.COLOR_BGR2RGB))\n",
        "  plt.title('Corresponding Epipolar Lines in right image')\n",
        "  plt.axis('off')\n",
        "\n",
        "  # Compute epilines corresponding to points in the right image, map to the left image\n",
        "  linesL = cv2.computeCorrespondEpilines(ptsR.reshape(-1, 1, 2), 2, fundamentalMatrix)\n",
        "  linesL = linesL.reshape(-1, 3)\n",
        "\n",
        "  # Draw the epipolar lines on the left image and the points on the right image\n",
        "  imgL_with_lines, imgR_with_points = draw_epipolar_lines(imgUndistL.copy(), imgUndistR.copy(), linesL, ptsR)\n",
        "  plt.figure(figsize=(20, 10))\n",
        "\n",
        "  # Plot the images with epipolar lines\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(cv2.cvtColor(imgR_with_points, cv2.COLOR_BGR2RGB))\n",
        "  plt.title('Points in right image')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(cv2.cvtColor(imgL_with_lines, cv2.COLOR_BGR2RGB))\n",
        "  plt.title('Corresponding Epipolar Lines in left image')\n",
        "  plt.axis('off')\n",
        "\n",
        "\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "03loHlW3zjrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7pTZiZ0IrmG"
      },
      "outputs": [],
      "source": [
        "# Stereo Rectification\n",
        "print(\"Computing undistortion and rectifications maps...\")\n",
        "'''\n",
        "Description: Computes the rectification transforms (R1, R2) and projection matrices (P1, P2) for both left and right cameras,\n",
        "given the stereo camera parameters. This function is key for aligning the images horizontally.\n",
        "Parameters:\n",
        "cameraMatrixL / cameraMatrixR: Intrinsic matrices of the left and right cameras, respectively.\n",
        "distL / distR: Distortion coefficients for the left and right cameras.\n",
        "frameSize: Size of the images.\n",
        "rot: Rotation matrix describing the orientation difference between the cameras.\n",
        "trans: Translation vector describing the relative position between the cameras.\n",
        "Return Values:\n",
        "R1 / R2: Rotation matrices for the left and right cameras, respectively.\n",
        "P1 / P2: New projection matrices for the left and right cameras, respectively.\n",
        "Q: The disparity-to-depth mapping matrix (used for depth map generation).\n",
        "roiL / roiR: The regions of interest for the rectified left and right images.\n",
        "'''\n",
        "R1, R2, P1, P2, Q, roiL, roiR = cv2.stereoRectify(cameraMatrixL, distL, cameraMatrixR, distR, frameSize, rot, trans)\n",
        "\n",
        "#  Undistortion and Rectification Maps\n",
        "'''\n",
        "Description: This function computes the undistortion and rectification transformation maps, which are later used to remap the images\n",
        "for rectification and undistortion.\n",
        "Parameters:\n",
        "cameraMatrixL / cameraMatrixR: Intrinsic matrices for the left and right cameras.\n",
        "distL / distR: Distortion coefficients for the left and right cameras.\n",
        "R1 / R2: Rectification transforms for the left and right cameras.\n",
        "P1 / P2: New projection matrices for the left and right cameras.\n",
        "frameSize: Size of the input images.\n",
        "cv2.CV_32F: Specifies the desired data type for the maps (floating-point format).\n",
        "Return Values:\n",
        "undistort_map_left / undistort_map_right: The maps that will be used to rectify the images.\n",
        "'''\n",
        "undistort_map_left = cv2.initUndistortRectifyMap(cameraMatrixL, distL, R1, P1, frameSize, cv2.CV_32F)\n",
        "undistort_map_right = cv2.initUndistortRectifyMap(cameraMatrixR, distR, R2, P2, frameSize, cv2.CV_32F)\n",
        "\n",
        "pathImgL = 'calib_data/robot/left/ZED_image_left_2.png'\n",
        "pathImgR = 'calib_data/robot/right/ZED_image_right_2.png'\n",
        "\n",
        "show_epipolar_geometry(pathImgL, pathImgR, undistort_map_left, undistort_map_right, method = 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine whether the epipolar geometry (particularly the fundamental matrix or essential matrix) is well-calibrated, you can observe how well the epipolar lines and corresponding points align across the stereo images. Here's how you can evaluate the quality of your epipolar geometry:\n",
        "\n",
        "1. Epipolar Lines and Corresponding Points:\n",
        "\n",
        "For each point in one image, the corresponding epipolar line in the other image should pass through the exact location of the matching point.\n",
        "\n",
        "If the epipolar geometry is correct, the points detected in one image should lie exactly on the epipolar lines computed in the other image.\n",
        "\n",
        "\n",
        "2. Visual Observation Criteria:\n",
        "\n",
        "When you visualize the epipolar geometry, the following indicators suggest that the matrices (fundamental matrix, essential matrix) are well-calibrated:\n",
        "\n",
        "- Good Calibration:\n",
        "For a point detected in one image, the epipolar line in the other image should pass through (or very close to) the corresponding matching point.\n",
        "The distance between the points and their corresponding epipolar lines should be minimal.\n",
        "\n",
        "- Lines are Aligned:\n",
        "The epipolar lines should be straight and should align horizontally after rectification. This ensures that corresponding points can be easily found by searching along the epipolar lines in a row-wise manner.\n",
        "\n",
        "3. Signs of Poor Calibration:\n",
        "\n",
        "If the epipolar lines miss their corresponding points by a significant margin, the fundamental matrix is not accurate.\n",
        "This can happen if the calibration is incorrect, or if there were errors in the detection of keypoints or matching.\n",
        "\n",
        "After stereo rectification, epipolar lines should be straight and should run horizontally across the images.\n",
        "If the lines are curved, misaligned, or do not match the corresponding points, this indicates a problem with the rectification process."
      ],
      "metadata": {
        "id": "9xOvMxJjAERm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLMpsLhX7ihk"
      },
      "outputs": [],
      "source": [
        "pathImgL = 'calib_data/robot/left/ZED_image_left_10.png'\n",
        "pathImgR = 'calib_data/robot/right/ZED_image_right_10.png'\n",
        "\n",
        "show_epipolar_geometry(pathImgL, pathImgR, undistort_map_left, undistort_map_right, method = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_inHpnyj399"
      },
      "source": [
        "## Stereo Reconstruction\n",
        "\n",
        "We have finally arrived to the most fun part of this project, what we have been waiting and preparing for: Stereo Reconstruction! Let's see how it all works.\n",
        "\n",
        "Overall Process of Stereo Reconstruction:\n",
        "- Load Stereo Images: The left and right stereo images are loaded from the file paths provided.\n",
        "- Stereo Rectification: Both stereo images are rectified so that corresponding points in both images lie on the same horizontal line.\n",
        "- Disparity Map Calculation: A disparity map is calculated, which represents the difference in pixel locations between corresponding points in the left and right images.The larger the disparity, the closer the object is to the camera.\n",
        "- Reprojection to 3D: The disparity map is used to compute a 3D point cloud, where each pixel is reprojected into 3D space.\n",
        "- Filtering and Visualization: The 3D point cloud is filtered based on the depth (z-axis) values and visualized using three.js."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb7he3Hncpfp"
      },
      "outputs": [],
      "source": [
        "def get_extents(vertices):\n",
        "    extents = np.zeros((3, 2))\n",
        "    extents[:, 0] = np.min(vertices, axis=0)\n",
        "    extents[:, 1] = np.max(vertices, axis=0)\n",
        "\n",
        "    return extents\n",
        "\n",
        "def sr(pathImgL, pathImgR, cmL, dL, cmR, dR, rM, tV, st, z_min = 0.0, z_max = 20.0):\n",
        "\n",
        "  # imgL = cv2.imread('captured_images/shoe/20240809/left/ZED_image_left_5.png')\n",
        "  # imgR = cv2.imread('captured_images/shoe/20240809/right/ZED_image_right_5.png')\n",
        "  imgL = cv2.imread(pathImgL)\n",
        "  imgR = cv2.imread(pathImgR)\n",
        "\n",
        "  # imgL = cv2.cvtColor(imgL, cv2.COLOR_BGR2RGB)\n",
        "  # imgR = cv2.cvtColor(imgR, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # Rectify the stereo cameras\n",
        "  R1, R2, P1, P2, Q, _, _ = cv2.stereoRectify(\n",
        "      cmL, dL, cmR, dR,\n",
        "      frameSize, rM, tV, flags=cv2.CALIB_ZERO_DISPARITY, alpha=-1\n",
        "  )\n",
        "\n",
        "  # Compute the rectification maps\n",
        "  map1_left, map2_left = cv2.initUndistortRectifyMap(cmL, dL, R1, P1, frameSize, cv2.CV_32FC1)\n",
        "  map1_right, map2_right = cv2.initUndistortRectifyMap(cmR, dR, R2, P2, frameSize, cv2.CV_32FC1)\n",
        "\n",
        "  # Rectify the images\n",
        "  left_rectified = cv2.remap(imgL, map1_left, map2_left, cv2.INTER_LINEAR)\n",
        "  right_rectified = cv2.remap(imgR, map1_right, map2_right, cv2.INTER_LINEAR)\n",
        "\n",
        "  # Optionally, save or display the rectified images\n",
        "  cv2.imwrite('left_rectified.png', left_rectified)\n",
        "  cv2.imwrite('right_rectified.png', right_rectified)\n",
        "\n",
        "  # Display the images with epipolar lines\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(left_rectified)\n",
        "  plt.title('Left Rectified Image ')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(right_rectified)\n",
        "  plt.title('Right Rectified Image')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  left_image = cv2.imread('left_rectified.png', cv2.IMREAD_GRAYSCALE)\n",
        "  right_image = cv2.imread('right_rectified.png', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "  left_rectified = cv2.cvtColor(left_rectified, cv2.COLOR_BGR2RGB)\n",
        "  right_rectified = cv2.cvtColor(right_rectified, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # Compute the disparity map\n",
        "  '''Description: Computes the disparity map using a stereo matching algorithm (e.g., StereoSGBM or StereoBM).\n",
        "  Parameters:\n",
        "  left_image / right_image: Grayscale rectified stereo images.\n",
        "  Return Value:\n",
        "  disparity_map: The calculated disparity map.\n",
        "  '''\n",
        "  disparity_map = st.compute(left_image, right_image).astype(np.float32) / 16.0\n",
        "\n",
        "  # Normalize the disparity map for visualization\n",
        "  disparity_map_normalized = cv2.normalize(disparity_map, None, 0, 255, cv2.NORM_MINMAX)\n",
        "  disparity_map_normalized = np.uint8(disparity_map_normalized)\n",
        "\n",
        "  # Optionally, save or display the disparity map\n",
        "  cv2.imwrite('disparity_map.png', disparity_map_normalized)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(disparity_map_normalized)\n",
        "  plt.title('Disparity Map')\n",
        "  plt.axis('off')\n",
        "\n",
        "\n",
        "  # Reproject points to 3D\n",
        "  '''Description: Converts the disparity map into 3D points.\n",
        "  Parameters:\n",
        "  disparity_map: Disparity map.\n",
        "  Q: Disparity-to-depth mapping matrix.\n",
        "  Return Value:\n",
        "  points_3D: A 3D point cloud of the scene.\n",
        "  '''\n",
        "  points_3D = cv2.reprojectImageTo3D(disparity_map, Q)\n",
        "\n",
        "  # Mask out points with zero disparity (infinite depth)\n",
        "  mask = disparity_map > disparity_map.min()\n",
        "  output_points = points_3D[mask]\n",
        "  output_colors = left_rectified[mask]\n",
        "\n",
        "  pcd = o3d.geometry.PointCloud()\n",
        "  pcd.points = o3d.utility.Vector3dVector(output_points)\n",
        "  pcd.colors = o3d.utility.Vector3dVector(output_colors.astype(float) / 255.0)\n",
        "\n",
        "  # Convert point cloud to numpy arrays for processing\n",
        "  points = np.asarray(pcd.points)\n",
        "  colors = np.asarray(pcd.colors)\n",
        "\n",
        "  # Create a mask for filtering based on the z values\n",
        "  mask = (points[:, 2] >= z_min) & (points[:, 2] <= z_max)\n",
        "\n",
        "  # Apply the mask to both points and colors\n",
        "  filtered_points = points[mask]\n",
        "  filtered_colors = colors[mask]\n",
        "\n",
        "  # Create a new point cloud from the filtered points and colors\n",
        "  filtered_pcd = o3d.geometry.PointCloud()\n",
        "  filtered_pcd.points = o3d.utility.Vector3dVector(filtered_points)\n",
        "  filtered_pcd.colors = o3d.utility.Vector3dVector(filtered_colors)\n",
        "\n",
        "  # Convert the point cloud to numpy arrays\n",
        "  points = np.asarray(filtered_pcd.points)\n",
        "  colors = np.asarray(filtered_pcd.colors)\n",
        "\n",
        "  n_vert = points.shape[0]\n",
        "  center = points.mean(axis=0)\n",
        "  extents = get_extents(points)\n",
        "\n",
        "  max_delta = np.max(extents[:, 1] - extents[:, 0])\n",
        "  camPos = [center[i] + 4 * max_delta for i in range(3)]\n",
        "  light_pos = [center[i] + (i+3)*max_delta for i in range(3)]\n",
        "\n",
        "  # Set up a scene and render it:\n",
        "  camera = p3js.PerspectiveCamera(position=camPos, fov=20,\n",
        "                                    )\n",
        "  camera.up = (0,0,1)\n",
        "\n",
        "  # Create a Three.js point cloud object\n",
        "  geometry = p3js.BufferGeometry(\n",
        "      attributes={\n",
        "          'position': p3js.BufferAttribute(points, normalized=False),\n",
        "          'color': p3js.BufferAttribute(colors, normalized=True)\n",
        "      }\n",
        "  )\n",
        "\n",
        "  material = p3js.PointsMaterial(vertexColors='VertexColors', size=0.03)\n",
        "  points_cloud = p3js.Points(geometry=geometry, material=material)\n",
        "\n",
        "  # Set up the scene and camera\n",
        "  # camera = p3js.PerspectiveCamera(position=[0, -10, 3], fov=75)\n",
        "  scene = p3js.Scene(children=[points_cloud, camera, p3js.AmbientLight()])\n",
        "  controller = p3js.OrbitControls(controlling=camera)\n",
        "\n",
        "  # Set up the renderer\n",
        "  renderer = p3js.Renderer(camera=camera, scene=scene, controls=[controller], width=800, height=600)\n",
        "\n",
        "  # Display the point cloud\n",
        "  display(renderer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Description: This function creates a **Stereo Block Matching (StereoBM)** object that computes disparity maps between rectified stereo image pairs using the block matching algorithm. The disparity map represents the difference in pixel locations of corresponding points between the left and right images, which is essential for depth estimation and 3D reconstruction.\n",
        "Parameters:\n",
        "- numDisparities: The disparity search range. It is the maximum disparity minus the minimum disparity. This parameter must be a positive integer divisible by 16 (e.g., 16, 32, 48, ...). A larger value allows the algorithm to detect objects at greater depths but increases computational complexity.\n",
        "- blockSize: The linear size of the square block window used to match pixel blocks between the left and right images. It must be an odd integer greater than or equal to 5 (e.g., 5, 7, 9, ...). A larger block size reduces noise but may miss finer details; a smaller block size preserves details but may introduce more noise.\n",
        "\n",
        "Return Values:\n",
        "- stereoBM: The StereoBM object that can be used to compute the disparity map between rectified stereo images using the `compute()` method.\n",
        "'''\n",
        "# Initialize the StereoBM object with optimized parameters\n",
        "stereo = cv2.StereoBM_create(\n",
        "    numDisparities=16*13,  # Must be divisible by 16. Allows for a wider range of depths.\n",
        "    blockSize=21          # Size of the block window (typically 5 to 21)\n",
        ")\n",
        "\n",
        "# Set additional StereoBM parameters\n",
        "stereo.setPreFilterCap(31)            # Controls the sensitivity of the pre-filter; handles high-contrast regions better.\n",
        "stereo.setMinDisparity(0)             # Minimum disparity.\n",
        "# stereo.setNumDisparities(16*5)        # Same as in the StereoBM initialization (must be divisible by 16).\n",
        "stereo.setTextureThreshold(1)        # Lower texture threshold for better matching in low-texture regions like glossy surfaces.\n",
        "stereo.setUniquenessRatio(1)         # Enforce uniqueness of matches to reduce false positives.\n",
        "stereo.setSpeckleWindowSize(1)      # Large speckle size to filter out small noisy regions in the disparity map.\n",
        "# stereo.setSpeckleRange(2)             # Maximum disparity variation within connected components for speckle filtering.\n",
        "# stereo.setDisp12MaxDiff(1)            # Maximum allowed difference in the left-right disparity map.\n",
        "\n",
        "pathImgL = 'calib_data/robot/left/ZED_image_left_1.png'\n",
        "pathImgR = 'calib_data/robot/right/ZED_image_right_1.png'\n",
        "sr(pathImgL, pathImgR, cameraMatrixL, distL, cameraMatrixR, distR, rot, trans, stereo, z_min = 1.0)"
      ],
      "metadata": {
        "id": "m8RThgRntdwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-1lppVdcvZq"
      },
      "outputs": [],
      "source": [
        "'''Description: This function creates a **Stereo Semi-Global Block Matching (StereoSGBM)** object, which computes a disparity map between rectified stereo image pairs using a semi-global block matching algorithm. This algorithm offers a balance between accuracy and performance, making it suitable for outdoor scenes with varying lighting and challenging surfaces, like glossy or reflective objects.\n",
        "Parameters:\n",
        "- minDisparity: The minimum possible disparity value. This parameter is often set to 0 if objects are expected to be close to the camera. If objects are further away, consider increasing this value to save computation time.\n",
        "- numDisparities: The disparity search range (must be divisible by 16). It is the maximum disparity minus the minimum disparity. A larger value (e.g., 16 * 13 = 208 in this case) allows detecting objects at greater depths, making it suitable for outdoor scenes.\n",
        "- blockSize: The size of the block window used for matching. A smaller block size (like 9) is used to capture fine details on surfaces like glossy or reflective objects, though it might be more sensitive to noise.\n",
        "- P1: The penalty for disparity changes by 1 between neighboring pixels. It is often set as `8 * channels * blockSize**2`, where `channels=3` for color images. This parameter helps to enforce smoothness in the disparity map.\n",
        "- P2: The penalty for larger disparity changes. Typically larger than `P1` and set as `32 * channels * blockSize**2`. This discourages large, abrupt disparity changes, contributing to a smoother disparity map.\n",
        "- disp12MaxDiff: The maximum allowed difference in the disparity map between the left and right images. A smaller value (like 1) ensures better consistency between the two disparity maps.\n",
        "- uniquenessRatio: A threshold that filters out ambiguous matches. It is the percentage by which the best match must be better than the second-best. A smaller value (e.g., 10) is suitable for low-texture surfaces, such as glossy black objects, which are harder to match.\n",
        "- speckleWindowSize: The size of the window used to filter out small, isolated noise regions in the disparity map. A smaller value (e.g., 150) avoids filtering out useful points in the scene while removing small speckles that aren't part of a larger, consistent disparity region.\n",
        "- speckleRange: The maximum disparity difference between neighboring pixels within a connected region to be considered part of the same object. A small value (e.g., 1) ensures that only small disparity variations are allowed within regions.\n",
        "- preFilterCap: This parameter adjusts the thresholding applied to pixel values before computing disparities. A value of 40 is good for outdoor scenes, helping to reduce the effect of high contrast or bright sunlight, but it can be adjusted between 30 and 63 depending on the lighting conditions.\n",
        "- mode: The computation mode for the algorithm. Using `cv2.STEREO_SGBM_MODE_SGBM_3WAY` improves the accuracy of the matching by running the algorithm in three different directions (left-right, top-bottom, and diagonal), which helps in outdoor scenes where lighting and textures may vary.\n",
        "Return Values\n",
        "- stereoSGBM: The stereo matcher object that computes the disparity map using the `compute()` method.\n",
        "'''\n",
        "# Create the stereo matcher\n",
        "bs = 9  # Smaller block size to capture more detail on glossy surfaces\n",
        "stereo = cv2.StereoSGBM_create(\n",
        "    minDisparity=0,  # Consider setting this to a positive value if objects aren't very close\n",
        "    numDisparities=16*13,  # Increase disparity range for outdoor scene, e.g., 192 or higher\n",
        "    blockSize=bs,  # Smaller block size to capture more detail on glossy surfaces\n",
        "    P1=8*3*bs**2,  # Adjust P1 and P2 based on blockSize\n",
        "    P2=32*3*bs**2,\n",
        "    disp12MaxDiff=1,  # Keep low for better consistency\n",
        "    uniquenessRatio=1,  # Reduce for low-texture objects like black glossy surfaces\n",
        "    speckleWindowSize=50,  # Reduce to avoid filtering out too many useful points\n",
        "    speckleRange=1,  # Keep small for low disparity variation\n",
        "    preFilterCap=40,  # Adjust depending on lighting (keep around 30-63 for outdoor scenes)\n",
        "    mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY  # Keep 3-way mode for higher accuracy\n",
        ")\n",
        "\n",
        "\n",
        "pathImgL = 'calib_data/robot/left/ZED_image_left_1.png'\n",
        "pathImgR = 'calib_data/robot/right/ZED_image_right_1.png'\n",
        "sr(pathImgL, pathImgR, cameraMatrixL, distL, cameraMatrixR, distR, rot, trans, stereo)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Initialize the StereoBM object\n",
        "stereo = cv2.StereoBM_create(\n",
        "  numDisparities=16*9,  # Must be divisible by 16\n",
        "  blockSize=21         # Size of the block window (typically 5 to 21)\n",
        ")\n",
        "\n",
        "# Initialize the StereoBM object with optimized parameters\n",
        "stereo = cv2.StereoBM_create(\n",
        "    numDisparities=16*13,  # Must be divisible by 16. Allows for a wider range of depths.\n",
        "    blockSize=21          # Size of the block window (typically 5 to 21)\n",
        ")\n",
        "\n",
        "# Set additional StereoBM parameters\n",
        "stereo.setPreFilterCap(31)            # Controls the sensitivity of the pre-filter; handles high-contrast regions better.\n",
        "stereo.setMinDisparity(0)             # Minimum disparity.\n",
        "# stereo.setNumDisparities(16*5)        # Same as in the StereoBM initialization (must be divisible by 16).\n",
        "stereo.setTextureThreshold(1)        # Lower texture threshold for better matching in low-texture regions like glossy surfaces.\n",
        "stereo.setUniquenessRatio(1)         # Enforce uniqueness of matches to reduce false positives.\n",
        "stereo.setSpeckleWindowSize(1)      # Large speckle size to filter out small noisy regions in the disparity map.\n",
        "# stereo.setSpeckleRange(2)             # Maximum disparity variation within connected components for speckle filtering.\n",
        "# stereo.setDisp12MaxDiff(1)            # Maximum allowed difference in the left-right disparity map.\n",
        "\n",
        "\n",
        "pathImgL = 'calib_data/robot/left/ZED_image_left_10.png'\n",
        "pathImgR = 'calib_data/robot/right/ZED_image_right_10.png'\n",
        "sr(pathImgL, pathImgR, cameraMatrixL, distL, cameraMatrixR, distR, rot, trans, stereo, z_min = 2)"
      ],
      "metadata": {
        "id": "-a9ExMbEu-cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxNDZAEBc02d"
      },
      "outputs": [],
      "source": [
        "# Create the stereo matcher\n",
        "bs = 9  # Smaller block size to capture more detail on glossy surfaces\n",
        "stereo = cv2.StereoSGBM_create(\n",
        "    minDisparity=0,  # Consider setting this to a positive value if objects aren't very close\n",
        "    numDisparities=16*13,  # Increase disparity range for outdoor scene, e.g., 192 or higher\n",
        "    blockSize=bs,  # Smaller block size to capture more detail on glossy surfaces\n",
        "    P1=8*3*bs**2,  # Adjust P1 and P2 based on blockSize\n",
        "    P2=32*3*bs**2,\n",
        "    disp12MaxDiff=1,  # Keep low for better consistency\n",
        "    uniquenessRatio=1,  # Reduce for low-texture objects like black glossy surfaces\n",
        "    speckleWindowSize=50,  # Reduce to avoid filtering out too many useful points\n",
        "    speckleRange=1,  # Keep small for low disparity variation\n",
        "    preFilterCap=40,  # Adjust depending on lighting (keep around 30-63 for outdoor scenes)\n",
        "    mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY  # Keep 3-way mode for higher accuracy\n",
        ")\n",
        "\n",
        "pathImgL = 'calib_data/robot/left/ZED_image_left_10.png'\n",
        "pathImgR = 'calib_data/robot/right/ZED_image_right_10.png'\n",
        "sr(pathImgL, pathImgR, cameraMatrixL, distL, cameraMatrixR, distR, rot, trans, stereo)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why are there so many holes on the robot?\n",
        "\n",
        "The presence of many holes in the reconstructed point cloud, particularly in areas corresponding to a black glossy object, is a common issue in stereo vision due to the following factors:\n",
        "\n",
        "1. Low Texture and Reflective Surfaces\n",
        "Black and glossy surfaces are typically low in texture and reflect a significant amount of light, making it difficult for the stereo matcher to find good correspondences between the left and right images.\n",
        "The stereo matching algorithm (like StereoSGBM or StereoBM) relies on finding distinct features in both images. Glossy surfaces cause specular reflections (bright spots), which may appear differently in the left and right images, causing mismatches or failed matches, resulting in \"holes\" in the disparity map.\n",
        "Black surfaces absorb light, leading to very low contrast, making it harder for the algorithm to identify keypoints or distinctive features for matching.\n",
        "2. Disparity Ambiguities\n",
        "On low-texture surfaces (like glossy or black surfaces), the stereo algorithm may have trouble distinguishing valid depth values from noise, as there is no strong texture for the algorithm to lock onto.\n",
        "This results in areas of zero disparity (or invalid disparity values), which translate into holes in the final point cloud.\n",
        "3. Lighting Conditions and Shadows\n",
        "Outdoor environments often introduce harsh lighting, such as strong sunlight or deep shadows, which further complicates stereo matching. Glossy objects tend to reflect sunlight, creating bright reflections on one image but not the other, which leads to a disparity mismatch.\n",
        "These reflections or shadowed areas are treated as \"ambiguous regions\" by the algorithm, resulting in missing depth information and holes in the point cloud.\n",
        "4. Specular Reflection\n",
        "When light reflects off a glossy surface at a certain angle, the reflection may show a strong highlight (specular reflection) in one image but not in the other. Since the reflection is not consistent between the two images, the stereo algorithm cannot find a reliable match, resulting in a mismatch in depth estimation and leading to holes."
      ],
      "metadata": {
        "id": "FVTWPv2ZQ9QD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bOtr6DhQtY2"
      },
      "source": [
        "## Factory Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vDy7HhPQsgr"
      },
      "outputs": [],
      "source": [
        "cameraMatrixFactoryL = np.array([[1.06419788e+03, 0.00000000e+00, 9.72515869e+02],\n",
        " [0.00000000e+00, 1.06419788e+03, 5.41038330e+02],\n",
        " [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])\n",
        "cameraMatrixFactoryR = np.array([[1.06419788e+03, 0.00000000e+00, 9.72515869e+02],\n",
        " [0.00000000e+00, 1.06419788e+03, 5.41038330e+02],\n",
        " [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])\n",
        "distFactoryL = np.array([[0., 0., 0., 0., 0.]])\n",
        "distFactoryR = np.array([[0., 0., 0., 0., 0.]])\n",
        "rotFactory = np.array([[1., 0., 0.],\n",
        " [0., 1., 0.],\n",
        " [0., 0., 1.]])\n",
        "transFactory = np.array([[-0.11969913], [0.], [0.]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJGCE84qTxCM"
      },
      "outputs": [],
      "source": [
        "R1, R2, P1, P2, Q, roiL, roiR = cv2.stereoRectify(cameraMatrixFactoryL, distFactoryL, cameraMatrixFactoryR, distFactoryR, frameSize, rotFactory, transFactory)\n",
        "undistort_map_left = cv2.initUndistortRectifyMap(cameraMatrixFactoryL, distFactoryL, R1, P1, frameSize, cv2.CV_32F)\n",
        "undistort_map_right = cv2.initUndistortRectifyMap(cameraMatrixFactoryR, distFactoryR, R2, P2, frameSize, cv2.CV_32F)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRIJzGkqUx3a"
      },
      "outputs": [],
      "source": [
        "pathImgL = 'calib_data/robot/left/ZED_image_left_2.png'\n",
        "pathImgR = 'calib_data/robot/right/ZED_image_right_2.png'\n",
        "\n",
        "show_epipolar_geometry(pathImgL, pathImgR, undistort_map_left, undistort_map_right, method = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is the factory calibration better? Look at the points and corresponding lines above and below this text, the lines cross almost exactly where the points on the other image are. If you look at the same points and lines for the calibration done in this colab (multiple cells above), you will notice that although the lines cross near where the points are, the result is not perfect. Conclusion: the factory calibration is very very accurate."
      ],
      "metadata": {
        "id": "iXAF8bdJ39m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pathImgL = 'calib_data/robot/left/ZED_image_left_10.png'\n",
        "pathImgR = 'calib_data/robot/right/ZED_image_right_10.png'\n",
        "\n",
        "show_epipolar_geometry(pathImgL, pathImgR, undistort_map_left, undistort_map_right, method = 2)"
      ],
      "metadata": {
        "id": "TwyqvwnWfKm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bad Calibration"
      ],
      "metadata": {
        "id": "A0WACPprRKhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the captured frames (should be in synch) (stereoLeft and stereoRight)\n",
        "CALIBRATION_IMAGES_PATH_LEFT = '/content/calib_data/bad-charuco/left/*.png'\n",
        "CALIBRATION_IMAGES_PATH_RIGHT = '/content/calib_data/bad-charuco/right/*.png'\n",
        "\n",
        "frameSize, imagesL, objpointsL, imgpointsL, idsL, cornersL, cameraMatrixBadL, distBadL, rvecsL, tvecsL, imagesR, objpointsR, imgpointsR, idsR, cornersR, cameraMatrixBadR, distBadR, rvecsR, tvecsR, rotBad, transBad, fundamentalMatrix, essentialMatrix = calibration(strict = False)"
      ],
      "metadata": {
        "id": "Vb5HhUMnRW7L",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute map to undistort and rectify images\n",
        "print(\"Computing undistortion and rectifications maps...\")\n",
        "R1, R2, P1, P2, Q, roiL, roiR = cv2.stereoRectify(cameraMatrixBadL, distBadL, cameraMatrixBadR, distBadR, frameSize, rotBad, transBad)\n",
        "undistort_map_left = cv2.initUndistortRectifyMap(cameraMatrixBadL, distBadL, R1, P1, frameSize, cv2.CV_32F)\n",
        "undistort_map_right = cv2.initUndistortRectifyMap(cameraMatrixBadR, distBadR, R2, P2, frameSize, cv2.CV_32F)\n",
        "\n",
        "pathImgL = 'calib_data/robot/left/ZED_image_left_2.png'\n",
        "pathImgR = 'calib_data/robot/right/ZED_image_right_2.png'\n",
        "\n",
        "show_epipolar_geometry(pathImgL, pathImgR, undistort_map_left, undistort_map_right, method = 1)"
      ],
      "metadata": {
        "id": "R1MlWV5mRuri",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathImgL = 'calib_data/robot/left/ZED_image_left_10.png'\n",
        "pathImgR = 'calib_data/robot/right/ZED_image_right_10.png'\n",
        "\n",
        "show_epipolar_geometry(pathImgL, pathImgR, undistort_map_left, undistort_map_right, method = 2)"
      ],
      "metadata": {
        "id": "eQJw97penDtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stereo matcher with relaxed parameters for bad calibration\n",
        "stereo = cv2.StereoSGBM_create(\n",
        "    minDisparity=0,\n",
        "    numDisparities=16*15,  # Large disparity range\n",
        "    blockSize=19,  # Large block size to try to force matches\n",
        "    P1=8 * 3 * 15**2,  # Reduced smoothness penalty\n",
        "    P2=16 * 3 * 15**2,  # Reduced smoothness penalty\n",
        "    disp12MaxDiff=50,  # High tolerance for disparity differences\n",
        "    uniquenessRatio=0,  # Allow less confident matches\n",
        "    speckleWindowSize=400,  # Large window size to allow more speckles\n",
        "    speckleRange=10,  # Allow more variation in speckle disparity\n",
        "    preFilterCap=20,  # Lower pre-filter cap\n",
        "    mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY  # 3-way matching for higher accuracy\n",
        ")\n",
        "\n",
        "\n",
        "pathImgL = 'calib_data/robot/left/ZED_image_left_1.png'\n",
        "pathImgR = 'calib_data/robot/right/ZED_image_right_1.png'\n",
        "sr(pathImgL, pathImgR, cameraMatrixBadL, distBadL, cameraMatrixBadR, distBadR, rotBad, transBad, stereo, z_min = -100, z_max = 100)"
      ],
      "metadata": {
        "id": "aZdrDvyXR6xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZjiznw8Stw5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Advanced_Calibration_Starter.ipynb",
      "collapsed_sections": [
        "wy7ftyucQomG",
        "pJORvlk2R0gs",
        "2AMay9zNSs1Y",
        "E0pVCbZzVaQK",
        "blV-5EXhBr2L",
        "v_t7lAo8Hu6V"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}